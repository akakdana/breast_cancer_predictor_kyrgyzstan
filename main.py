# -*- coding: utf-8 -*-
"""Breast_Cancer_Predictor_Kyrgyzstan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kTaiF0O9pOlCfRRqg5G5kop9dc9F21NN

# Библиотеки
"""

!pip install category_encoders

!pip install optuna

!pip install catboost

import pandas as pd
import numpy as np

from sklearn.preprocessing import OneHotEncoder
import category_encoders as ce

import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score


from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold

import optuna

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from catboost import CatBoostClassifier

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

"""# Загрузка данных"""

data = pd.read_csv('/content/breast_cancer_predictor_kyrgyzstan.csv')
data

data.info()

data.columns

"""# Чистка данных

Избавляемся от ненужных колонок:
"""

data = data.drop(columns=['Дата регистрации', 'Дата создания', 'Осмотр у гинеколога', 'Обследования', 'День цикла'])

"""**Возраст(Лет)**

Смотрим, где значения в столбце 'Возраст(Лет)' больше 100:
"""

data[data['Возраст(Лет)'] > 100]

"""Корректируем значения в столбце 'Возраст(Лет)':"""

data['Возраст(Лет)'] = data['Возраст(Лет)'] % 100
data = data.drop(data[data['Возраст(Лет)'] <= 2].index)

"""**BIRADS**

Делим BIRADS(Общее) на две категории: malignant и benign:

c 1 по 3 — изменений нет, либо новообразования доброкачественные (benign);

c 4 по 5 — существуют признаки злокачественности патологии (malignant).
"""

data['BIRADS(Общее)'] = data['BIRADS(Общее)'].astype('str')
benign = ['1', '2', '3']
malignant = ['4C', '4', '4A', '5', '4B']
data['BIRADS(Общее)'] = ['malignant' if x in malignant else 'benign' for x in data['BIRADS(Общее)']]

"""Создаем новую колонку 'BIRADS_encoded' и преобразуем значения в колонке 'BIRADS(Общее)' в бинарные значения:"""

data['BIRADS_encoded'] = [1 if x == 'malignant' else 0 for x in data['BIRADS(Общее)']]
data['BIRADS_encoded'].value_counts()

"""**Наследственность**

Преобразуем значения в колонке 'Наследственность' в бинарные значения:
"""

data['Наследственность'] = data['Наследственность'].replace('Нет', 0)
data['Наследственность'] = data['Наследственность'].replace('Родство (двоюродная сестра, тётя)', 1)
data['Наследственность'] = data['Наследственность'].replace('Близкое родство (мама, родная сестра, бабушка по материнской линии)', 1)

data['Наследственность'].value_counts()

"""**Аборты**

Преобразуем колонку 'Аборты' в бинарные значения: 0 - нет, 1 - да:
"""

data['Аборты'] = [1 if x == 'Да' else 0 for x in data['Аборты']]
data['Аборты'].value_counts()

"""**Менструальные циклы**"""

data['Менструальные циклы'].value_counts()

"""Используем функцию get_dummies из библиотеки pandas, чтобы преобразовать столбец 'Менструальные циклы':"""

encoded_df = pd.get_dummies(data['Менструальные циклы'])
data = pd.concat([data, encoded_df], axis=1)

data.columns

"""**Результаты маммографии**

Преобразование категориального признака 'Результаты маммографии' на основе целевого признака 'BIRADS_encoded', используя Target Encoding:
"""

encoder = ce.TargetEncoder(cols=['Результаты маммографии'])

encoded_df = encoder.fit_transform(data['Результаты маммографии'], data['BIRADS_encoded'])
encoded_df = encoded_df.rename(columns={'Результаты маммографии': 'Результаты маммографии_encoded'})
data = pd.concat([data, encoded_df], axis=1)

data['Результаты маммографии_encoded'].value_counts()

"""# Визуализация

**Возраст**

1. Распределим данные по возрастным группам: младше 45 лет, от 45 лет до 65 лет, и старше 65 лет:
"""

bins = [data['Возраст(Лет)'].min(), 45, 65, data['Возраст(Лет)'].max()]
labels = ['Мин. - 45 лет', '45-65 лет', '65 - Макс. лет']

age_groups = pd.cut(data['Возраст(Лет)'], bins=bins, labels=labels)

count_by_age_group = age_groups.value_counts()

plt.pie(count_by_age_group.values, labels=count_by_age_group.index, autopct='%1.2f%%')
plt.title('Распределение данных по возрастным группам:')
plt.show()

"""Благодаря этой визуализации можно быстро определить примерное распределение данных по возрасту. Наибольшая доля данных приходится на группу "45-65 лет", что может указывать на средний возраст наблюдаемой выборки. Группы "Мин. - 45 лет" и "65 - Макс. лет" составляют оставшуюся долю данных.

2. Распределим данные возрастов с использованием гистограммы с учетом разделения данных по ширине:
"""

plt.subplots(figsize=(15, 5))
sns.histplot(data = data, x = 'Возраст(Лет)', multiple = 'dodge', shrink = .7)
plt.title('Распределение данных по возрасту:')
plt.show()

"""Визуализация представляет собой гистограмму, которая показывает распределение данных по возрасту среди исследуемой аудитории.

По высоте столбцов мы можем определить, сколько наблюдений входит в каждую возрастную категорию. В данном случае, мы видим, что самые частые значения возраста, находится в середине диапазона возрастов. Остальное количество записей снижается, что может указывать на естественное сокращение числа представителей с более младшими или старшими возрастами.

3. Рассмотрим взаимосвязь между возрастом и общим числом беременностей на графике рассеяния с цветовой схемой:
"""

plt.subplots(figsize=(15, 5))
sns.scatterplot(x = data['Возраст(Лет)'], y = data['Общее число беременностей'], hue = data['Аборты'], palette='viridis')
plt.title('Взаимосвязь между возрастом и общим числом беременностей:')
plt.show()

"""Точечная диаграмма позволяет нам визуально оценить взаимосвязь между возрастом и общим числом беременностей, а также выявить возможные зависимости с учетом фактора 'Аборты'.

На основе предоставленной визуализации мы можем сделать следующие наблюдения и сформулировать выводы:

Возрастные точки, расположенные слева от 30 на горизонтальной оси, имеют ниже средние значения на вертикальной оси (число беременностей).
Это указывает на тенденцию к меньшему количеству беременностей у женщин до 30 лет.

Точки на графике сосредоточены преимущественно в диапазоне возраста от 35 до 75 лет. В этом диапазоне можно заметить значительное увеличение числа беременностей, что может свидетельствовать о более высокой вероятности беременности в этом возрастном диапазоне.

По мере увеличения возраста после 75 лет, точки становятся менее плотными, это указывает на тенденцию к уменьшению числа беременностей у женщин старше 75 лет.

4. Изучим, как возраст распределен среди различных категорий в столбце 'Менструальные циклы':
"""

plt.subplots(figsize=(15, 5))
sns.histplot(data = data, x = 'Возраст(Лет)', hue = 'Менструальные циклы' ,multiple = 'dodge', shrink = .9)
plt.title('Распределение возраста для каждой категории "Менструальные циклы":')
plt.show()

"""Взглянув на график, мы можем сделать следующие наблюдения:

Гистограмма позволяет сравнивать распределение возраста для каждой категории 'Менструальные циклы'. Каждый столбчатый набор групп представляет определенную категорию, а цветовая группировка указывает на отличия в распределении возраста между этими категориями.

Высота столбцов на графике отображает количество наблюдений (частоту) в каждой возрастной группе. Более высокие столбцы указывают на большее количество наблюдений в этой группе возраста.

**BIRADS (Breast Imaging Reporting and Data System)**

5. Распределим данные по типам BIRADS: benign - доброкачественная, malignant - злокачественная:
"""

birads_counts = data['BIRADS(Общее)'].value_counts()

plt.figure(figsize=(8, 6))
birads_counts.plot(kind='pie', autopct='%1.1f%%')
plt.ylabel('')
plt.title('Распределение по типам опухали:')
plt.show()

"""Круговая диаграмма наглядно демонстрирует распределение данных по двум типам опухолей. Сектор "benign" с наибольшим процентом указывает на наиболее часто встречающийся тип опухоли среди записей. Сектор "malignant" с меньшим процентом представляет тип опухоли, который составляет небольшую часть общего числа записей.

6. Посмотрим зависит ли тип BIRADS от общего числа беременностей:
"""

fig = px.box(data, x='Общее число беременностей', y='BIRADS(Общее)', color='Наследственность', title = 'Зависимость типа опухоли от общего числа беременностей:')
fig.show()

"""Ящиковая диаграмма (box plot) позволяет наглядно отобразить статистические характеристики данных и сравнивать распределение значений между различными категориями. Цветовые различия в ящиках указывают на различные значения переменной 'Наследственность'. Каждая категория 'Наследственность' имеет свой цвет.

На данном графике видно, что общее количество беременностей не вляет на тип опухоли у женщин.

7. Рассмотрим тип BIRADS в зависимости от возраста:
"""

plt.subplots(figsize=(15, 3))
sns.scatterplot(x = data['Возраст(Лет)'], y = data['BIRADS(Общее)'])
plt.title('Зависимость типа опухали от возраста:')
plt.show()

"""График позволяет увидеть, как распределены наблюдения по возрасту и типу BIRADS. Мы можем определить, что у женщин в возрасте до 35 лет преобладает тип доброкачественной РМЖ(рак молочной железы).

После 35 лет точки на графике распределены между обоими типами BIRADS (доброкачественными и злокачественными). Это указывает на то, что после 35 лет возраст не вляет на категорию BIRADS, в равном количестве тип доброкачественной и тип злокачественной РМЖ.

8. Рассмотрим тип BIRADS в зависимости от возраста и наследственности:
"""

fig = px.box(data, x = 'Возраст(Лет)', y = 'BIRADS(Общее)', color = 'Наследственность', title = 'Тип BIRADS в зависимости от возраста и наследственности:')
fig.show()

"""Цветовые различия в ящиках указывают на различные значения переменной 'Наследственность', что позволяет сравнивать распределение типов BIRADS для каждого значения 'Возраст(Лет)'. Как и на предыдущем графике мы видим, что оба типа BIRADS встречаются одинаково в возрастном диапозоне от 35 до 75 лет. Но мы также можем сделать вывод, что у женщин со злокачественной РМЖ в наследственности наблюдается большее количество случаев, чем у тех, у кого доброкачественная РМЖ.

"""

data.columns

"""9. Посмотрим на степень линейной взаимосвязи между парами признаков:"""

features = ['Возраст(Лет)', 'Наследственность', 'BIRADS_encoded', 'Результаты маммографии_encoded',
                      'Общее число беременностей', 'Число естественных родов',
                      'Число родов методом кесарева сечения', 'Аборты', 'Первые роды в возрасте',
                      'Общее число случаев рождения живого ребенка',
                      'Общее число случаев рождения мертвого ребенка', 'День цикла', 'Менопауза',
                      'Нарушение менструального цикла']

correlation_matrix = data[features].corr(numeric_only=True)
correlation_matrix

"""Данный код вычисляет матрицу корреляции для выбранных признаков (features). В переменной 'features' содержатся названия столбцов, которые являются числовыми и для которых мы хотим посчитать корреляцию. В матрице корреляции мы видим числовые значения, представляющие коэффициенты корреляции между выбранными признаками.

Высокие значения коэффициентов корреляции (близкие к 1 или -1) указывают на сильную линейную зависимость между признаками.

Низкие значения коэффициентов корреляции (близкие к 0) говорят о слабой или отсутствующей линейной связи между признаками.

Корреляция не подразумевает причинно-следственную связь между признаками. Высокая корреляция может указывать на взаимосвязь или совпадение в данных, но не всегда означает, что один признак вызывает изменение другого.

10. Создадим красочную и информативную тепловую карту (heatmap), которая поможет лучше понять взаимосвязи между признаками в наших данных:
"""

plt.figure(figsize=(12, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='RdYlGn')
plt.title('Correlation Matrix')
plt.show()

"""На тепловой карте мы видим матрицу, где каждый элемент содержит значение коэффициента корреляции между соответствующими признаками.

Корреляция близка к 1 (яркий зеленый цвет) указывает на сильную положительную линейную связь между признаками.

Корреляция близка к -1 (яркий красный цвет) указывает на сильную отрицательную линейную связь между признаками.

Корреляция близка к 0 (более бледные цвета) указывает на отсутствие или слабую линейную связь между признаками.

В нашей матрице мы видим сильную положительную связь (0.84) между результатами маммографии и типом BIRADS, это говорит о том, что от результата маммографии зависит тип РМЖ. А также сильная отрицательная связь видна между признаком "Возраст" и "День цикла", из этого можно сделать вывод, что с возрастом у женщин меняется организм, иногда происходят нарушения цикла.

# Построение моделей

Мы будем использовать несколько алгоритмов машинного обучения: *Logistic Regression, Random Forest Classifier, Gradient Boosting Classifier, CatBoostClassifier*.

Чтобы точно понять, какая модель наилучшим образом подойдет для нашей задачи предсказания типа BIRADS, мы будем использовать несколько подходов и методов оценки производительности моделей. Вот описание каждого подхода:

**Без кросс-валидации**: В данном подходе мы разделим данные на обучающую и тестовую выборки. Затем обучим модель на обучающей выборке и оценим ее производительность, используя тестовую выборку. Метрики оценки помогут нам определить, какая модель лучше справляется с предсказанием типа BIRADS на отложенной выборке.

**С кросс-валидацией**: В данном подходе мы применим кросс-валидацию, чтобы получить более стабильные оценки производительности моделей. Данные разделяются на несколько "фолдов", и модель обучается и тестируется на разных комбинациях данных. Это помогает учесть возможные вариации в данных и предоставляет более надежную оценку производительности. Мы будем использовать метрику средний F1-Score для всех фолдов.

**Без кросс-валидации с использованием оптимизации**: В данном подходе мы применим оптимизацию гиперпараметров моделей, используя Optuna. Мы будем выбирать оптимальные значения гиперпараметров, которые максимизируют производительность модели на отложенной выборке.

**С кросс-валидацией с использованием оптимизации**: Этот подход объединяет два предыдущих. Мы применяем кросс-валидацию с использованием оптимизации гиперпараметров. Это позволяет нам оценить производительность моделей с более надежными оценками и выбрать оптимальные параметры для каждой модели.

В результате, мы получим оценки производительности для каждой модели и сможем выбрать наилучшую модель на основе выбранной метрики оценки (F1-Score). Важно провести адекватное сравнение моделей с помощью различных подходов и методов оценки, чтобы принять наилучшее решение для данной задачи.

*Без кросс-валидации:*

Шаг 1: Разделение данных на признаки (X) и целевую переменную (y)
"""

X = data[['Возраст(Лет)', 'Наследственность', 'Аборты', 'День цикла',
       'Менопауза', 'Нарушение менструального цикла', 'Результаты маммографии_encoded']]
y = data['BIRADS_encoded']

"""Шаг 2: Разделение данных на обучающую и тестовую выборки"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train: ', X_train.shape)
print('X_test: ', X_test.shape)
print('y_train: ', y_train.shape)
print('y_test: ', y_test.shape)

"""Шаг 3: Создание, тренировка и предсказание объектов моделей

Logistic Regression:
"""

logreg_model = LogisticRegression(max_iter=1000)
logreg_model.fit(X_train,y_train)

y_pred_logreg = logreg_model.predict(X_test)

f1 = f1_score(y_test, y_pred_logreg)
print('F1_score: ', f1)

"""Random Forest Classifier:"""

rfc_model = RandomForestClassifier()
rfc_model.fit(X_train,y_train)

y_pred_rfc = rfc_model.predict(X_test)

f1 = f1_score(y_test, y_pred_rfc)
print('F1_score: ', f1)

"""GradientBoostingClassifier:"""

gbc_model = GradientBoostingClassifier()
gbc_model.fit(X_train,y_train)

y_pred_gbc = gbc_model.predict(X_test)

f1 = f1_score(y_test, y_pred_gbc)
print('F1_score: ', f1)

"""CatBoostClassifier:"""

catbc_model = CatBoostClassifier()
catbc_model.fit(X_train,y_train)

y_pred_catbc = catbc_model.predict(X_test)

f1 = f1_score(y_test, y_pred_catbc)
print('F1_score: ', f1)

"""Исходя из результатов без кросс-валидации, можно сделать следующие выводы:

Logistic Regression показала очень низкую производительность с F1-Score равным 0.0. Это означает, что данная модель не смогла правильно предсказать ни одного класса.

Random Forest Classifier показал отличную производительность с F1-Score равным 0.943. Это говорит о том, что модель случайного леса очень хорошо справляется с задачей классификации и способна делать точные предсказания.

Gradient Boosting Classifier также показала хороший результат с F1-Score равным 0.918. Градиентный бустинг обладает хорошей способностью к обобщению и высокой точностью предсказаний.

Cat Boost Classifier также продемонстрировал хороший результат с F1-Score равным 0.918, что делает его сравнимым с градиентным бустингом. CatBoost обрабатывает категориальные признаки автоматически, что упрощает процесс подготовки данных.

Итог: Без кросс-валидации, случайный лес показывает лучшие результаты по сравнению с другими моделями. Однако, чтобы более точно определить, какая модель является лучшей, рекомендуется провести кросс-валидацию для оценки производительности каждой модели.

*С кросс валидацией:*

Шаг 1: Разделение данных на признаки (X) и целевую переменную (y)
"""

X = data[['Возраст(Лет)', 'Наследственность', 'Аборты', 'День цикла',
       'Менопауза', 'Нарушение менструального цикла', 'Результаты маммографии_encoded']]
y = data['BIRADS_encoded']

"""Шаг 2: Создаем разбиения на кросс-валидацию с 5 фолдами и оцениваем производительность моделей на всех фолдах.

Logistic Regression:
"""

logreg_model = LogisticRegression(max_iter=1000, random_state = 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
logreg_scores = cross_val_score(logreg_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', logreg_scores.mean())
print('Individual Fold F1-Scores:', logreg_scores)

"""Random Forest Classifier:"""

rfc_model = RandomForestClassifier(random_state = 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
rfc_scores = cross_val_score(rfc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', rfc_scores.mean())
print('Individual Fold F1-Scores:', rfc_scores)

"""Gradient Boosting Classifier:"""

gbc_model = GradientBoostingClassifier(random_state = 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
gbc_scores = cross_val_score(gbc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', gbc_scores.mean())
print('Individual Fold F1-Scores:', gbc_scores)

"""Cat Boost Classifier:"""

catbc_model = CatBoostClassifier(random_state = 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
catbc_scores = cross_val_score(catbc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', catbc_scores.mean())
print('Individual Fold F1-Scores:', catbc_scores)

"""По результатам кросс-валидации для различных моделей, мы получили следующие средние значения F1-Score:

Логистическая регрессия: Mean F1-Score: 0.0

Random Forest Classifier: Mean F1-Score: 0.9160253493448343

Gradient Boosting Classifier: Mean F1-Score: 0.9151652621312166

Cat Boost Classifier: Mean F1-Score: 0.9039324116743472

Вывод:

Логистическая регрессия показала очень низкое среднее значение F1-Score, равное 0.0. Это может указывать на то, что данная модель не смогла хорошо обучиться и правильно классифицировать примеры. Возможно, для данной задачи логистическая регрессия не является подходящим алгоритмом или требуется дополнительная настройка параметров.

Случайный лес и Градиентный бустинг показали сравнимо высокие средние значения F1-Score, примерно равные 0.92. Это говорит о том, что обе модели хорошо справляются с задачей классификации и предсказывают тип BIRADS с высокой точностью. Оба алгоритма могут быть рассмотрены как перспективные варианты для этой задачи.

CatBoost также продемонстрировал хорошую производительность со средним значением F1-Score около 0.90. Это свидетельствует о том, что CatBoost также является хорошим алгоритмом для классификации типа BIRADS и может быть рассмотрен в качестве альтернативы другим моделям.

*Без кросс-валидации с использованием оптимизации:*

Шаг 1: Разделение данных на признаки (X) и целевую переменную (y)
"""

X = data[['Возраст(Лет)', 'Наследственность', 'Аборты', 'День цикла',
       'Менопауза', 'Нарушение менструального цикла', 'Результаты маммографии_encoded']]
y = data['BIRADS_encoded']

"""Шаг 2: Разделение данных на обучающую и тестовую выборки"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('X_train: ', X_train.shape)
print('X_test: ', X_test.shape)
print('y_train: ', y_train.shape)
print('y_test: ', y_test.shape)

"""Шаг 3: Подбор параметров, тренировка и предсказание для каждой модели

Logistic Regression:
"""

def objective(trial):
    params = {
        'penalty' : trial.suggest_categorical('penalty', ['l1', 'l2']),
        'C': trial.suggest_loguniform('C', 0.1, 1),
        'max_iter': trial.suggest_int('max_iter', 100, 1000),
        'random_state': 2
    }

    logreg_model = LogisticRegression(**params, verbose=False, solver='liblinear')
    logreg_model.fit(X_train, y_train)

    y_pred_logreg = logreg_model.predict(X_test)
    f1 = f1_score(y_test, y_pred_logreg)

    return f1

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
best_f1 = study.best_value

print('Best Params:', best_params)
print('Best F1:', best_f1)

"""Random Forest Classifier:"""

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 10, 400),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'random_state': 2
    }

    rfc_model = RandomForestClassifier(**params, verbose=False)
    rfc_model.fit(X_train, y_train)

    y_pred_rfc = rfc_model.predict(X_test)
    f1 = f1_score(y_test, y_pred_rfc)

    return f1

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
best_f1 = study.best_value

print('Best Params:', best_params)
print('Best F1:', best_f1)

"""Gradient Boosting Classifier:"""

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),
        'random_state': 2
    }

    gbc_model = GradientBoostingClassifier(**params, verbose=False)
    gbc_model.fit(X_train, y_train)

    y_pred_gbc = gbc_model.predict(X_test)
    f1 = f1_score(y_test, y_pred_gbc)

    return f1

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
best_f1 = study.best_value

print('Best Params:', best_params)
print('Best F1:', best_f1)

"""Cat Boost Classifier:"""

def objective(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'depth': trial.suggest_int('depth', 3, 10),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
        'random_strength': trial.suggest_int('random_strength', 1, 10),
        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 10.0),
        'border_count': trial.suggest_int('border_count', 1, 255),
        'random_state': 2
    }

    model_catbc = CatBoostClassifier(**params, verbose=False)
    model_catbc.fit(X_train, y_train)

    y_pred_catbc = model_catbc.predict(X_test)
    f1 = f1_score(y_test, y_pred_catbc)

    return f1

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
best_f1 = study.best_value

print('Best Params:', best_params)
print('Best F1:', best_f1)

"""На основании предоставленных результатов без кросс-валидации с использованием Optuna для каждой модели, можно сделать следующие выводы:

Логистическая регрессия, Random Forest Classifier и Gradient Boosting Classifier показали одинаково высокое значение метрики F1, равное 0.976. Это означает, что все три модели достигли отличных результатов на тестовой выборке без использования кросс-валидации, и они хорошо справились с предсказанием типа BIRADS.

Модель Cat Boost Classifier также показала хороший результат с метрикой F1 равной 0.965. Она немного уступила другим моделям, но все равно имеет хорошую производительность.

Важно отметить, что результаты без кросс-валидации могут быть подвержены переобучению, так как модели могут быть настроены на конкретные особенности обучающего набора данных. Поэтому, чтобы получить более объективную оценку производительности моделей, рекомендуется использовать кросс-валидацию.

Для более точного сравнения моделей и выбора наилучшей модели, рекомендуется выполнить оптимизацию с использованием кросс-валидации для каждой модели. Это поможет оценить, как хорошо модели обобщаются на новых данных и устойчивы к вариативности данных.

*С кросс-валидацией и использованием уже подобранных гиперпараметров:*

Шаг 1: Разделение данных на признаки (X) и целевую переменную (y)
"""

X = data[['Возраст(Лет)', 'Наследственность', 'Аборты', 'День цикла',
       'Менопауза', 'Нарушение менструального цикла', 'Результаты маммографии_encoded']]
y = data['BIRADS_encoded']

"""Шаг 2. Создаем разбиения на кросс-валидацию с 5 фолдами и оцениваем производительность моделей на всех фолдах.

Logistic Regression:
"""

logreg_model = LogisticRegression(penalty = 'l1', C = 0.2024039299616781, max_iter = 196, solver='liblinear', random_state = 2)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
logreg_scores = cross_val_score(logreg_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', logreg_scores.mean())
print('Individual Fold F1-Scores:', logreg_scores)

"""Random Forest Classifier:"""

rfc_model = RandomForestClassifier(random_state = 2, n_estimators = 18, max_depth = 3, min_samples_split = 8, min_samples_leaf = 7)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
rfc_scores = cross_val_score(rfc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', rfc_scores.mean())
print('Individual Fold F1-Scores:', rfc_scores)

"""Gradient Boosting Classifier:"""

gbc_model = GradientBoostingClassifier(random_state = 2, n_estimators = 188, learning_rate = 0.007171978647042725, max_depth = 9,
                                       min_samples_split = 6, min_samples_leaf = 10, subsample = 0.5481244446213769, max_features = 'auto')
kf = KFold(n_splits=5, shuffle=True, random_state=42)
gbc_scores = cross_val_score(gbc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', gbc_scores.mean())
print('Individual Fold F1-Scores:', gbc_scores)

"""Cat Boost Classifier:"""

catbc_model = CatBoostClassifier(random_state = 2, iterations = 108, depth = 7, learning_rate = 0.001629581056117214,
                                 random_strength = 6, bagging_temperature = 2.523452173324185, border_count = 54)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
catbc_scores = cross_val_score(catbc_model, X, y, cv=kf, scoring='f1')

print('Mean F1-Score:', catbc_scores.mean())
print('Individual Fold F1-Scores:', catbc_scores)

"""На основе предоставленных результатов с использованием кросс-валидации и использование уже подобранных гиперпараметров с помощью Optuna для каждой модели, можно сделать следующие выводы:

Модель Cat Boost Classifier показала самый высокий результат по метрике F1, равный 0.939. Это означает, что Cat Boost Classifier с оптимальными параметрами из Optuna лучше всего справляется с предсказанием типа BIRADS на кросс-валидации среди всех рассмотренных моделей.

Модели логистической регрессии, Random Forest Classifier и Gradient Boosting Classifier также показали хорошие результаты, с метриками F1 около 0.926-0.928.

Все четыре модели продемонстрировали хорошие показатели, и все они имеют высокую способность к предсказанию типа BIRADS на кросс-валидации.

Итог: запуск моделей с кросс-валидацией и оптимизацией гиперпараметров позволяет получить более объективную оценку производительности и выбрать наилучшую модель для нашей задачи.
"""